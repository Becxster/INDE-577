<!doctype html>
<html>
<head>
<meta charset="utf-8">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/styles/default.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css">
<link rel="stylesheet" href="file:////Users/beckedwards/.vscode/extensions/goessner.mdmath-2.7.4/themes/default/style.css">

</head>
<body class="markdown-body">
<h1 dir="auto" id="perceptron">Perceptron</h1>
<p dir="auto">The perceptron is a type of artificial neural network and one of the simplest forms of a neural network used for binary classification tasks. It is a supervised learning algorithm that learns a linear decision boundary to separate two classes. The perceptron algorithm was invented by Frank Rosenblatt in 1957 and is the foundation of more complex neural networks.</p>
<h2 dir="auto" id="how-the-perceptron-works">How the Perceptron Works</h2>
<p dir="auto">The perceptron consists of input features, weights, a bias term, and an activation function. The input features are multiplied by their corresponding weights, and the bias term is added to the result. The sum is then passed through an activation function to produce the output.</p>
<h3 dir="auto" id="mathematical-representation">Mathematical Representation</h3>
<p dir="auto">The output of the perceptron can be represented mathematically as:</p>
<p dir="auto">[ y = \phi(\mathbf{w} \cdot \mathbf{x} + b) ]</p>
<p dir="auto">Where:</p>
<ul dir="auto">
<li dir="auto">( \mathbf{x} ) is the input feature vector.</li>
<li dir="auto">( \mathbf{w} ) is the weight vector.</li>
<li dir="auto">( b ) is the bias term.</li>
<li dir="auto">( \phi ) is the activation function (usually the unit step function).</li>
</ul>
<h3 dir="auto" id="activation-function">Activation Function</h3>
<p dir="auto">The activation function used in a perceptron is typically the unit step function, which outputs 1 if the input is greater than or equal to 0, and 0 otherwise:</p>
<p dir="auto">[ \phi(z) = \begin{cases}
1 &amp; \text{if } z \geq 0 \
0 &amp; \text{if } z &lt; 0
\end{cases} ]</p>
<h3 dir="auto" id="training-the-perceptron">Training the Perceptron</h3>
<p dir="auto">The perceptron is trained using the perceptron learning rule, which updates the weights and bias based on the prediction error. The update rule is as follows:</p>
<p dir="auto">[ \mathbf{w} \leftarrow \mathbf{w} + \eta (y_{\text{true}} - y_{\text{pred}}) \mathbf{x} ]
[ b \leftarrow b + \eta (y_{\text{true}} - y_{\text{pred}}) ]</p>
<p dir="auto">Where:</p>
<ul dir="auto">
<li dir="auto">( \eta ) is the learning rate.</li>
<li dir="auto">( y_{\text{true}} ) is the true label.</li>
<li dir="auto">( y_{\text{pred}} ) is the predicted label.</li>
</ul>
<h3 dir="auto" id="diagram-of-a-perceptron">Diagram of a Perceptron</h3>
<p dir="auto">Below is a diagram illustrating the structure of a perceptron:</p>
<pre><code class="code-line language-markdown" dir="auto">+-------------------+
| Input Features    |
| (x1, x2, ..., xn) |
+-------------------+
<span class="hljs-code">         |
         v
+-------------------+
| Weights (w1, w2,  |
| ..., wn)          |
+-------------------+
         |
         v
+-------------------+
| Weighted Sum      |
| (Σ wi * xi + b)   |
+-------------------+
         |
         v
+-------------------+
| Activation        |
| Function (φ)      |
+-------------------+
         |
         v
+-------------------+
| Output (y)        |
+-------------------+
</span></code></pre>
<h2 dir="auto" id="applications-of-perceptron">Applications of Perceptron</h2>
<p dir="auto">The perceptron is used in various applications, including:</p>
<ul dir="auto">
<li dir="auto">Binary classification tasks</li>
<li dir="auto">Pattern recognition</li>
<li dir="auto">Image processing</li>
<li dir="auto">Document classification</li>
</ul>
<p dir="auto">Despite its simplicity, the perceptron is a powerful tool for understanding the basics of neural networks and serves as a building block for more complex models.</p>

</body>
</html>